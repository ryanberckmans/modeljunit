<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <title>Model-Based Testing with ModelJUnit</title>
  </head>

  <body>
    <h2>Overview</h1>

    <p>
    ModelJUnit is a Java library for model-based testing.  The basic
    idea is that you write an abstract model of your system under test
    (SUT), then you generate lots of tests from that model.
    ModelJUnit is usually used to do <em>online</em> (on-the-fly)
    testing, where the tests are executed on the SUT as they are
    generated.
    </p>

    <p>
    Some advantages of model-based testing are that it can be quicker
    than writing a test suite by hand, can give systematic coverage of
    the behaviours of the model and can make it easier to support
    requirements evolution (update the model and regenerate the
    tests).
    </p>

  <h2>How to do Model-Based Testing</h2>
    <p>
    There are four basic steps to using model-based testing:
    <ol>
      <li><strong>Design a model of your SUT.</strong> In ModelJUnit,
         this model is written as a Java class that implements the
         {@link net.sourceforge.czt.modeljunit.FsmModel} interface.
	 The current state of the SUT is modelled by the private data variables
         of this class and the operations of the SUT are modelled by
         the <em>action methods</em> of this class.
         For example, see the 
         {@link net.sourceforge.czt.modeljunit.examples.SimpleSet} model.
         </li>
      <li><strong>Connect your Model to your SUT.</strong>
         Add a pointer to an SUT object into your model class.
         Then add some 'adaptor' code to the action methods of your model so
         that each action method tests one or more of the SUT operations.
         This means that as the automatic test generator traverses
         your model, it will be calling SUT operations and checking
         their results.
         For example, see the 
         {@link net.sourceforge.czt.modeljunit.examples.SimpleSetWithAdaptor}
         and {@link net.sourceforge.czt.modeljunit.examples.SmartSetAdaptor}
         classes, which add adaptor code to the SimpleSet model.
         </li>
      <li><strong>Generate some Tests.</strong>
         Create a test suite by passing an instance of your model
         to one of the Tester classes, for example,
         {@link net.sourceforge.czt.modeljunit.GreedyTester}.
         Then you can generate online (on-the-fly) test sequences of 
         varying length using its generate(N) method.  You can also
         connect various model coverage metrics to the tester so that
         you can see how thoroughly the model has been tested.
         See the examples in the
         <a href="examples/package-summary.html">examples</a>
         package for more detail.
         </li>
      <li><strong>Analyze any Test Failures.</strong>
         Differences between your model and your SUT are reported
         as test failures.  You should analyze these to determine
         if they are caused by an SUT error, or a model/adaptor error.
         </li>
    </ol>
    </p>

  <h2>Test Generation Algorithms</h2>
    <p>
      Here is a brief overview of most of the test generation
      algorithms that ModelJUnit provides.  These are all subclasses
      of the {@link net.sourceforge.czt.modeljunit.Tester} class.
      Many of the test generation algorithms use randomness to explore
      the graph of the model, but by default the random number
      generator is usually created with a fixed seed so that the test
      generation will be predictable and the same test results will be
      obtained each session.  You can use the {@link
      net.sourceforge.czt.modeljunit.Tester#setRandom(Random)} method
      in the Tester class if you want to use different seeds.  For
      example, <code>tester.setRandom(new Random())</code> will make
      <code>tester</code> generate different tests (which may expose
      different bugs) in each test generation session.  (If you do
      this, I suggest that you print out the seed, since it can be
      quite annoying to see a test failure, and then not know how to
      reproduce that failure later.)
    </p>
      
    <p>
      {@link net.sourceforge.czt.modeljunit.RandomTester} does a
      random walk around the graph.  At each state it randomly chooses
      one of the enabled transitions.  {@link
      net.sourceforge.czt.modeljunit.GreedyTester} is a subclass of
      RandomTester that also does a random walk, but gives preference
      to the unexplored transitions.  This means that it covers the
      transitions of the model more quickly than RandomTester, then
      its behaviour becomes identical to RandomTester once all
      transitions out of each state have been tested.
    </p>

    <p>
      {@link net.sourceforge.czt.modeljunit.LookaheadTester} is like
      GreedyTester, but more sophisticated, because it looks ahead
      several transitions to see where there are unexplored areas
      and then tries to go towards those areas.  It allows the lookahead
      depth and several other parameters to be set to give some control
      over the search.
    </p>

    <p>
      {@link net.sourceforge.czt.modeljunit.AllRoundTester} can be
      used as a wrapper around any other test generation algorithm.
      It terminates each generated test sequence once a given number
      of loops are detected (one by default).  So it is helpful for
      generating all-round-trips test sets.
    </p>

</tr>
</table>

    <hr>
    <address><a href="mailto:marku@cs.waikato.ac.nz">marku@cs.waikato.ac.nz</a></address>
<!-- Created: Thu Oct 19 22:48:21 NZST 2006 -->
<!-- hhmts start -->
Last modified: Sat Dec 15 22:13:29 NZDT 2007
<!-- hhmts end -->
  </body>
</html>
